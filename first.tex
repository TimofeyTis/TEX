% This is a simple sample document.  For more complicated documents take a look in the excersice tab. Note that everything that comes after a % symbol is treated as comment and ignored when the code is compiled.

\documentclass{article} % \documentclass{} is the first command in any LaTeX code.  It is used to define what kind of document you are creating such as an article or a book, and begins the document preamble


\usepackage[T2A]{fontenc}       %                              РУСИФИКАЦИЯ
\usepackage[utf8]{inputenc}     %
\usepackage[russian]{babel}     %
\usepackage{pscyr}              %

\usepackage{graphicx}           %                              для картинок
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[left=2cm,right=2cm, top=2cm,bottom=2cm,bindingoffset=0.2cm]{geometry}



\title{ЗАДАЧА MARL ДЛЯ СВЕТОФОРА НА ПЕРЕКРЁСТКЕ}
\author{Т.И. Тисленко}

\graphicspath{{pics/}}

\date{\today} % Sets date for date compiled

% The preamble ends with the command \begin{document}
\begin{document} % All begin commands must be paired with an end command somewhere
    \maketitle% creates title using information in preamble (title, author, date)
    \newtheorem{Def}{}[section]	
	\newtheorem{Ex}{}
    %\section{} % creates a section
	В работе рассматривается задача оптимизации сети светофоров с точки зрения уменьшения задержек трафика автомобилей. 
    Объектом исследования являются временные интервалы, в течение которых машины не перемещаются. 
    Предметом является согласование действий сети светофоров по управлению процессом выбора задержки сигнала: зеленого, красного. 
    В качестве метода решения поставленной задачи предложен мультиагентный подход. 
    Каждый интеллектуальный агент решает задачу целесообразности выбора того или иного сигнала. 
    Предложены математические модели системы, учитывающие возможность совместного выполнения задачи.
    
    
 %------------------------------------------------------------------------------------------     
    \section{Основные определения}

    \begin{Def}\label{IA}
        \textbf{Интеллектуальным агентом} называется метаобъект, наделенный долей субъектности,
        взаимодействующий с другими агентами и средой, выполняющий определенные функции для достижения поставленных целей.
    \end{Def}
    
    \begin{Def}\label{EN}
        \textbf{Средой} называется множество объектов, не принадлежащих агенту. 
    \end{Def}

    \begin{Def}\label{RES}
        \textbf{Задачами/Ресурсами} называется объект, распределяемый агентами в ходе достижения их целей.
    \end{Def}

    \begin{Def}\label{MS}
        \textbf{Мультиагентная система} – совокупность взаимосвязанных агентов.
    \end{Def}

    \begin{Def}\label{RL}
        \textbf{RL(Reinforcement Learning)} — Обучение с подкреплением, где
    в роли учителя выступает среда. Как правило, RL используется для
    одного агента в среде, чтобы максимизировать его долгосрочную
    (накопитльную, кумулятивную) награду. Модель среды – Марковский
    процесс принятия решения - MDP(Markov decision process), предпола-
    гается, что эта среда стационарна, т.е. состояние среды зависит только
    от действий агента. Самая общая модель обучения одного агента в среде — Q-обучение.
     Q-обучаемый агент учится оптимальному сопоставлению действия $a$(action) состоянию окружающей среды $s$(state) на
    основе кумулятивных наград $r(s, a)$ (reward).
    \end{Def}
%------------------------------------------------------------------------------------------  
	\section{Постановка задачи MARL}
	Рассмотрим модель обучения одного агента.  

    В качестве агента выступает светофор. Ресурсами такой агент не располагает. 

    Среда --- перекресток с машинами, где на отрезках дорог за 100м до стоп-линий засекается время. 

    Состояние среды отражает активность фазы светофора. Обозначим их фаза0, фаза1.
    Пространство состояний S = $\{$ $s_0$ = <<активна фаза0>> , $s_1$ = <<активна фаза1>>  $\}$

    В момент времени $t_k$ активна фаза светофора $S_k$, суммарное засеченое всех машин, 
    проходящих через отрезок дороги называется задержкой на фазе $S_k$ 
    %\medskip

	Множество решений 
	A = $\{$ $a_0$ = <<оставить фазу>> , $a_1$ = <<сменить фазу>> $\}$

	Будем считать, что функция вознаграждения всецело определяется текущим состоянием, выбранной стратегией и состоянием, 
    в которое перейдет процесс на следующем шаге:
	
	$r(s_k, a_0)$ = задержка на фазе $s_k$
		
	$r(s_k, a_1)$ = задержка на фазе $s_{1-k}, k=1,2.$

	$p(i, k; j)$ вероятность того, что система из состояния $i$ при выборе решения $k$ попадает в состояние $j$,
     полностью определяется состоянием, в которое переходит процесс.
	
	
	$V^*(s)$ --- функция суммарных внешних доходов от оптимальной политики в состоянии s
    \begin{equation}\label{VAL}
	    V^*(s)  = 
        \max_{a(\cdot )} \sum_{t =0 }^{\infty} \gamma ^t r(s_t, a_t).
    \end{equation}
	
    Исходя из описания задачи уравнение Вальда - Беллмана ~\cite{LEC} для управляемого марковского процесса с конечным числом действий и состояний имеет вид:
	\begin{equation}\label{BELLMAN}
	    V^*(s)  = 
        \max_{a\in A} \left\{ \sum_{s'\in S} p(s, a; s') (r(s, a; s') +  \gamma V^* (s') )\right\} =\\
        \max_{a \in \{a_0,a_1\}} \left\{ \sum_{s'\in \{s_0, s_1\}} p(s, a; s') (r(s, a; s') +  \gamma V^* (s') )\right\}
        .
    \end{equation}
    вероятности в правой части неизвестны
    
	Пусть Q-функция имеет следующий вид:
    \begin{equation}
        Q(s, a) = \sum_{s'\in S} p(s, a; s')(r(s, a; s')) +  \gamma V^* (s').
    \end{equation}
    тогда: 
    \begin{equation}
	    V^*(s) = \max_{a\in A}  Q(s, a).
    \end{equation}
    
    \begin{equation}
        Q(s, a) = \sum_{s'\in S}  p(s, a; s')(r(s, a; s') + \gamma \max_{a'\in A}  Q(s', a')).
    \end{equation}
        
        
    Итак, $Q = \{ Q(s, a)\}_{s \in S, a' \in A}$ , можно записать итеративно $Q_{t+1} = A(Q_t)$, где $A \colon \mathbb{R} _{\infty}^1 \to  \mathbb{R} _{\infty}^1$ --- сжимающее отображение.
    
    \begin{equation} 
        \begin{split}
        \rho ((A \circ Q_1)(s, a), (A \circ Q_2)(s, a)) & = 
           \max_{a'\in A, s'\in S}| \sum_{s'\in S}  p(s, a; s')(r(s, a; s') + \gamma \max_{a'\in A}  Q_1(s', a')) - \\
           \sum_{s'\in S}  p(s, a; s')(r(s, a; s') + \gamma \max_{a'\in A}  Q_2(s', a'))|  
           & \leq 
           \max_{a'\in A, s'\in S} |\gamma \max_{a'\in A}  Q_1(s', a')) - \gamma \max_{a'\in A}  Q_2(s', a'))| = \\
           & = 
        \gamma \rho (Q_1(s, a), Q_2(s, a)) ,  \gamma \in (0; 1)
        \end{split}
    \end{equation}
   \par\medskip
   Последовательность $\{ Q_t \}$ представляет собой приближенные решения $AQ = Q$ эффективный способ оценки точности которого:
    \begin{equation} 
        \rho(Q_{t_n}(s, a), Q_{t_0}(s, a)) = \rho((A^n \circ Q_{t_0})(s, a), Q_{t_0}(s, a)) \leq \frac{\gamma ^n \rho(Q_{t_1}(s, a), Q_{t_0}(s, a))}{ 1 - \gamma} = \frac{\gamma ^n \rho((A \circ Q_{t_0})(s, a),Q_{t_0}(s, a))}{ 1 - \gamma}     
    \end{equation}
    
    Идея Q-обучения заключается в оценке невычислимой правой части:
	\begin{equation} \label{Qiteration}
        Q_{t+1}(s, a) =  Q_t(s, a) + \alpha _t (s, a) \left(r(s, a) +  \gamma \max_{a'\in A}  Q_t(s', a') - Q_t(s, a)\right)
    \end{equation}
    где $s'$--– положение процесса на шаге $t+1$, если на шаге $t$ процесс был в состоянии $s$ и было выбрано действие $a$. Если на шаге $t$ процесс находился в состоянии $s$ и было выбрано действие a, то $0 \leq \alpha _t(s, a) \leq 1$, иначе $\alpha _t(s, a) = 0$.
    
    \begin{equation}
	    V_t ^*(s) = \max_{a\in A}  Q_t(s, a).
    \end{equation}

    Оказывается, что если используемая стратегия $a(s)$ приводит к тому, что с вероятностью 1 каждая пара $(s, a)$ будет бесконечное число раз встречаться на бесконечном горизонте наблюдения, то из отмеченного выше условия сжимаемости при
    \begin{equation}
        \sum _{t=0} ^\infty {\alpha _t (s, a)} = \infty, 
        \sum _{t=0} ^\infty {\alpha _t (s, a)^2} \leq \infty
    \end{equation}
    удет следовать сходимость (с вероятностью 1)  процесса \ref{Qiteration}

    \begin{equation}
	    V ^*(s) = \max_{a\in A}  \lim _{t \to + \infty}Q_t(s, a).
    \end{equation}
    \begin{equation}
	    a_t(s) = \arg  \max_{a' \in A} Q_t(s, a')
    \end{equation}
    
    
%------------------------------------------------------------------------------------------    
    \section{Цель работы}
    Целью работы являлось ознакомление с подходами, позволяющими оптимизировать процесс выбора сигнала светофора, с учетом текущей загрузки транспорта, с точки зрения минимизации задержки. В работе получены следующие результаты:

    \begin{enumerate}
        \item Математическая модель процесса выбора фазы светофора, отличающаяся учетом текущего расположения светофоров и их загрузки и позволяющая сформулировать оптимизационные задачи, целью которых является минимизация задержки трафика автомобилей.
        \item Структура мультиагентной системы, включающая в себя единственного агента – светофор, обеспечивающая наиболее эффективное распараллеливание всей задачи на подзадачи, которые будут решены агентом.
    \end{enumerate}
%------------------------------------------------------------------------------------------    
   %\section{Список литературы} 


    \begin{thebibliography}{3}
        \bibitem{LEC}
            Лекции по случайным процессам : учебное пособие /
            А. В. Гасников, Э. А. Горбунов, С. А. Гуз и др. ; под ред.
            А. В. Гасникова. – <<Москва>> : МФТИ, 2019. – 285 с.
            ISBN 978-5-7417-0710-4
        \bibitem{BOOK}
            Марковские процессы принятия решений. /
            Майн X., Осаки С. 
            Главная редакция физико-математической литературы издательства «Наука», 
            1977.  - 176 с. 
            УДК 519.283
        %
    \end{thebibliography}



\end{document} % This is the end of the document