% This is a simple sample document.  For more complicated documents take a look in the excersice tab. Note that everything that comes after a % symbol is treated as comment and ignored when the code is compiled.

\documentclass{article} % \documentclass{} is the first command in any LaTeX code.  It is used to define what kind of document you are creating such as an article or a book, and begins the document preamble
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc} 
\usepackage[russian]{babel} 
\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage[left=2cm,right=2cm, top=2cm,bottom=2cm,bindingoffset=0.2cm]{geometry}



\title{МУЛЬТИАГЕНТНЫЕ ТЕХНОЛОГИИ В УПРАВЛЕНИИ МОДЕЛЯМИ СЛОЖНЫХ СИСТЕМ}
\author{Т.И. Тисленко}

\graphicspath{{pics/}}

\date{\today} % Sets date for date compiled

% The preamble ends with the command \begin{document}
\begin{document} % All begin commands must be paired with an end command somewhere
    \maketitle% creates title using information in preamble (title, author, date)
    
    \newtheorem{Def}{}[section]	
	\newtheorem{Ex}{}
	
    \section{Вступление} % creates a section
	В работе рассматривается задача оптимизации сети светофоров с точки зрения уменьшения задержек трафика автомобилей. Объектом исследования являются временные интервалы, в течение которых машины не перемещаются. Предметом является согласование действий сети светофоров по управлению процессом выбора задержки сигнала: зеленого, желтого, красного. В качестве метода решения поставленной задачи предложен мультиагентный подход. Каждый интеллектуальный агент решает задачу целесообразности выбора того или иного сигнала. Предложены математические модели системы, учитывающие возможность совместного выполнения задачи.
    
    
    
    \textbf{Интеллектуальным агентом} называется метаобъект, наделенный долей субъектности, взаимодействующий с другими агентами и средой, выполняющий определенные функции для достижения поставленных целей.
     
    
    
    \textbf{Средой} называется множество объектов, не принадлежащих агенту. 
     
    
    
    \textbf{Задачами/Ресурсами} называется объект, распределяемый агентами в ходе достижения их целей.
     
   
    
    \textbf{Мультиагентная система} – совокупность взаимосвязанных агентов.
    
    
    
    Рассмотрены 2 задачи: \textbf{MARA} и \textbf{ MARL}.
    
    
    
%------------------------------------------------------------------------------------------  
	\section{Постановка задачи MARL}
	
	
	\textbf{RL(Reinforcement Learning)} — Обучение с подкреплением, где
    в роли учителя выступает среда. Как правило, RL используется для
    одного агента в среде, чтобы максимизировать его долгосрочную
    (накопитльную, кумулятивную) награду. Модель среды – Марковский
    процесс принятия решения - MDP(Markov decision process), предпола-
    гается, что эта среда стационарна, т.е. состояние среды зависит только
    от действий агента. Самая общая модель обучения одного агента в среде — Q-обучение. Q-обучаемый агент учится оптимальному сопоставлению действия a(action) состоянию окружающей среды s(state) на
    основе кумулятивных наград r(s,a) (reward).
	\par\medskip
	
	Опишем управляемый марковский процесс с конечным числом состояний и действий:
	


    Пространство состояний светофора S = $\{$ $s_0$ = активна фаза0, $s_1$ = активна фаза1  $\}$


	Множество решений 
	A = $\{$ $a_0$ = оставить фазу, $a_1$ = сменить фазу $\}$

		

	
	Будем считать, что функция вознаграждения всецело определяется текущим состоянием, выбранной стратегией и состоянием, в которое перейдет процесс на следующем шаге:
	

	r($s_k$,$a_0$)= задержка на фазе $s_k$
		
	r($s_k$, $a_1$) = задержка на фазе $s_{1-k}$, k=1,2.

	
	
	
	
	
		
	

	$p(i, k; j)$ вероятность того, что система из состояния i при выборе решения k попадает в состояние j, полностью определяется состоянием, в которое переходит процесс.
	
	
	$V^*(s)$ --- функция суммарных внешних доходов от оптимальной политики в состоянии s
	
    Уравнение Вальда - Беллмана имеет вид:
	\begin{Def}
	\[
	V^*(s) = \max_{a\in A} \sum_{s'\in S} p(s, a; s') (r(s, a; s')) +  \gamma V^* (s').
    \]
    \end{Def}
    
    \newpage
	Пусть Q-функция имеет следующий вид:
    \begin{Def}
	\[
	Q(s, a) = \sum_{s'\in S} p(s, a; s')(r(s, a; s')) +  \gamma V^* (s').
    \]
    \end{Def}
    тогда: 
    \begin{Def}
	\[
	V^*(s) = \max_{a\in A}  Q(s, a).
    \]
    \end{Def}
    
    \begin{Def}
	\[
	Q(s, a) = \sum_{s'\in S}  p(s, a; s')(r(s, a; s') + \gamma \max_{a'\in A}  Q(s', a'))
    \]
    \end{Def}
    
    
    Итак, $Q = \{ Q(s, a)\}_{s \in S, a' \in A}$ , можно записать итеративно $Q_{t+1} = A(Q_t)$, где $A \colon \rm I\!R _{\infty}^1 \to \rm I\!R _{\infty}^1$ --- сжимающее отображение.
    
    \begin{Def} 
    \[
        \begin{split}
        \rho((A \circ Q_1)(s, a), (A \circ Q_2)(s, a)) & = 
           \max_{a'\in A, s'\in S}| \sum_{s'\in S}  p(s, a; s')(r(s, a; s') + \gamma \max_{a'\in A}  Q_1(s', a')) - \\
           \sum_{s'\in S}  p(s, a; s')(r(s, a; s') + \gamma \max_{a'\in A}  Q_2(s', a'))|  
           & \leq 
           \max_{a'\in A, s'\in S} |\gamma \max_{a'\in A}  Q_1(s', a')) - \gamma \max_{a'\in A}  Q_2(s', a'))| = \\
           & = 
        \gamma \rho (Q_1(s, a), Q_2(s, a)) ,  \gamma \in (0; 1)
        \end{split}
    \]
    \end{Def}
   
   
   
   \par\medskip
   Последовательность $\{ Q_t \}$ представляет собой приближенные решения $AQ = Q$ эффективный способ оценки точности которого:
    \begin{Def} 
        \[
        \rho(Q_{t_n}(s, a), Q_{t_0}(s, a)) = \rho((A^n \circ Q_{t_0})(s, a), Q_{t_0}(s, a)) \leq \frac{\gamma ^n \rho(Q_{t_1}(s, a), Q_{t_0}(s, a))}{ 1 - \gamma} = \frac{\gamma ^n \rho((A \circ Q_{t_0})(s, a),Q_{t_0}(s, a))}{ 1 - \gamma} 
        \]
    \end{Def}
    
    Идея Q-обучения заключается в оценке невычислимой правой части:
	\begin{Def} \label{Qiteration}
        \[
        Q_{t+1}(s, a) =  Q_t(s, a) + \alpha _t (s, a) \left(r(s, a) +  \gamma \max_{a'\in A}  Q_t(s', a') - Q_t(s, a)\right)
        \]
    \end{Def}
    где $s'$--– положение процесса на шаге t+1, если на шаге t процесс был в состоянии s и было выбрано действие a. Если на шаге t процесс находился в состоянии s и было выбрано действие a, то $0 \leq \alpha _t(s, a) \leq 1$, иначе $\alpha _t(s, a) = 0$.
    \begin{Def}
	\[
	V_t ^*(s) = \max_{a\in A}  Q_t(s, a).
    \]
    \end{Def}
    
    Оказывается, что если используемая стратегия a(s) приводит к тому, что с вероятностью 1 каждая пара (s, a) будет бесконечное число раз встречаться на бесконечном горизонте наблюдения, то из отмеченного выше условия сжимаемости при
    \begin{Def}
	\[
	\sum _{t=0} ^\infty {\alpha _t (s, a)} = \infty, 
	\sum _{t=0} ^\infty {\alpha _t (s, a)^2} \leq \infty
    \]
    \end{Def}
    будет следовать сходимость (с вероятностью 1)  процесса \ref{Qiteration}
    
    \begin{Def}
	\[
	V ^*(s) = \max_{a\in A}  \lim _{t \to + \infty}Q_t(s, a).
    \]
    \end{Def}
    \begin{Def}
	\[
	a_t(s) = arg  \max_{a' \in A} Q_t(s, a')
    \]
    \end{Def}
    
    
%------------------------------------------------------------------------------------------    
    \section{Примеры задач RL}
    \textbf{Биореактор.}
    
		Обучение с подкреплением применяется времени от времени для 
определения температуры и скорости перемешивания  для биореактора (большого 
чана питательных веществ и бактерий, используемых для производства полезных 
химикатов). Действия в таком приложении могут быть нацелены на  температуру и 
скорость перемешивания, которые передаются на низкоуровневую систему 
контроля, это, в свою очередь, непосредственно активирует отопительные 
элементы и моторы, чтобы достичь целей. Состояния, вероятно, будут термопарой 
с другими считывающими сенсорами, возможно, фильтруемыми и с задержками, плюс 
символьными входами описывающими ингриденты в чане и  целевом химическом 
веществе. Вознаграждения должны быть помоментными измерениями скорости, с 
которой биореактор производит полезное химическое вещество. Заметим, что 
здесь каждое состояние --- список/вектор со считывающего сенсора и 
символьных входов, и каждое действие --- вектор, состояящий из целевой 
температуры и скорости вращения. Это типично для задачи \textbf{RL} иметь 
состояния и действия с такими структурированными представлениями. Награды, с 
другой стороны, всегда одиночные числа.

    \par\medskip\medskip
    \textbf{Робот-мусорщик.}
    
		Мобильный робот занятый сбором банок из под газировки в офисной 
среде. Он имеет сенсоры для обнаружения банок и руку, и зажим, которые могут 
поднять и поместить их бортовую корзину; он питается от заряжаемой батареи. 
Система управления робота имеет компоненты для интерпретации информации с 
датчиков для навигации, для контроля руки и зажима. Высокоуровневые решения о 
том, как искать банки сделаны с помощью обучаемом агента на 
основе текущего уровня зарядки. Этот агент решает когда робот (1) должен 
активно искать банки , (2) оставаться неподвижным и ждать когда, кто-то принесет ему банку, (3) возвращаться на зарядную станцию. 
У агента есть три действия, и состояние среды определено зарядом батареи. 
Награды нулевые большую часть времени, и положительными, когда робот находит пустую банку или отрицательными, если батарея 
близка к разряду. В этом примере, обучаемый агент не является 
всем роботом. Состояния, которые он отслеживает, описывают условия внутри 
робота,а не состояние всей внешней среды вокруг робота. Таким образом, среда 
агента включает в себя часть робота, а также его внешнюю среду.
    
    
    

%------------------------------------------------------------------------------------------  
    \section{Постановка задачи MARA}
    
    \textbf{MARA (Multy Agent Resource Allocation)} --- задача раcпределения ресурсов/заданий между агентами.
    В ходе решения задачи MARA агенты могут обмениваться задачами для достижения наилучшего распределения, при котором их общие затраты на выполнение минимальны. 
    
    \par\medskip
    $A=\left\{ A_{i}\right\}_{i=1,..,N}$--- множество агентов;
    
    $T=\left\{ T_{i}\right\}_{i=1,..,N}$--- множество заданий/ресурсов находящихся в распоряжении агента $A_i\in A$;
    
    $c_{i}:2^T\mapsto {\rm I\!R}$--- функции стоимости выполнения набора заданий для агента $A_i\in A$;
    
    $( T_{1}^{init},\dots ,T_{N}^{init})$--- начальное распределение заданий;
    
    \par\medskip\medskip
    Требуется найти такое распределение \[
    ( T_{1}^*,\dots ,T_{N}^*): \sum_A c_i(T_i^*) = \min_{2^T} (\sum_A c_i(T_i))
    \]
    
    
    \par\medskip\medskip\medskip
    \textbf{MC (Merge Cost, рус. подрядная стоимость)} называется число соответствующее разности стоимости задачи для Агента и стоимости подряда.
    \begin{Def}\label{MCADD}
    \[
        MC^{add} (T^{contract} | T_i) = c_i(T_i)-c_i(T_i - T^{contract})
    \]
    \end{Def}
    \begin{Def}\label{MCREM}
    \[
        MC^{remove} (T^{contract} | T_i) = -c_i(T_i)+c_i(T_i \bigcup T^{contract})
    \]
    \end{Def}
    
    \newpage
    \textbf{Адгоритм решения}
    \begin{enumerate}
        \item Каждый агент расчитывает подрядную стоимость \ref{MCADD} для каждой задачи из своего списка задач.
        \item Первый агент просматривает список задач второго \ref{MCREM}, определяет наиболеее выгодную для него задачу.
        \item Второй агент просматривает список задач первого \ref{MCREM}, определяет наиболеее выгодную для него задачу.
        \item Принимается решение суммарная выгода которого наибольшая.
        
    \end{enumerate}
%------------------------------------------------------------------------------------------  
    \section{Примеры задач MARA}
    \textbf{Индустриальные закупки}
    
    Существующие запросы к инструментам :
    \begin{enumerate}
        \item требуется производить мониторинг цен на рынке товаров/услуг в режиме реального времени, 
        \item рассматривать отдельно возможность доставки провайдером,
        \item вести переговоры с возмоностью бартера,
        \item выбирать лучшее предложение.
    \end{enumerate}
    
    Вызовы :
    \begin{enumerate}
        \item требуется полная информация о предпочтениях покупателя/продавца,
        \item различиях конфигурации товара,
        \item возможности переговоров об изменении атрибутов,
        \item о механизме выбора победителя.
    \end{enumerate}    
    \par\medskip\medskip\medskip
    \textbf{Спутники наблюдения Земли}   
    
    Существующие запросы к инструментам :
    \begin{enumerate}
        \item окупаемость вложений,
        \item задействованность всех спутников,
        \item эффективность использования топлива,
        \item учет запрещенных зон в маршруте.
    \end{enumerate}
    
\end{document} % This is the end of the document